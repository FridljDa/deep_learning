{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 Word2Vec & Seq2Seq Models\n",
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys,  json, pickle, tqdm, random, warnings \n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt#, seaborn as sns\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils import data\n",
    "\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from gensim.test.utils import common_texts\n",
    "import gensim, gensim.downloader as gensim_api\n",
    "from gensim import corpora\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_documents\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "pl.seed_everything(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: you can find other `gensim` datasets [here](https://github.com/RaRe-Technologies/gensim-data#datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/default/Google Drive/currentDocumants/studies/Master/6.Semester/deep_learning/pset/pset3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_DIR = os.path.abspath('.')\n",
    "\n",
    "W2V_FILE = os.path.join(SAVE_DIR, 'word2vec.pth')\n",
    "\n",
    "CORPUS = 'text8'\n",
    "\n",
    "TXT_FILE = os.path.join(SAVE_DIR, f'{CORPUS}.pkl')\n",
    "\n",
    "OVERWRITE = False\n",
    "\n",
    "VEC_SIZE = 50\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "SAVE_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch you text corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: generate your own word-vector embedding using a text corpus of _your_ choice of at least **500** words. You may use `gensim`'s corpora.\n",
    "\n",
    "Feel free to play with hyperparmaters in the notebook such as context size and embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gensim_api.load('text8')  # download the corpus and return it opened as an iterable\n",
    "#length of Dataset corpus\n",
    "#corpus_vec = [word for _, word in enumerate(corpus)]\n",
    "#corpus.shape\n",
    "#len(corpus)\n",
    "#get type of corpus\n",
    "#type(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does_corpus_exist = os.path.isfile(\"data/Faust.txt\")\n",
    "\n",
    "#if not does_corpus_exist or OVERWRITE:\n",
    "#    corpus = gensim_api.load(CORPUS)\n",
    "#    with open(TXT_FILE, 'wb') as f:\n",
    "#        pickle.dump(corpus, f)\n",
    "\n",
    "#else:\n",
    "#    with open(TXT_FILE, 'rb') as f:\n",
    "#        corpus = pickle.load(f)\n",
    "\n",
    "#corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your Word2Vec Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: you are free to create your own Word2Vec model. You are welcome to use `gensim`. However for the pset you must at least _train_ a Word2Vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "does_model_exist = os.path.isfile(W2V_FILE)\n",
    "\n",
    "if not does_model_exist or OVERWRITE:\n",
    "    w2v = Word2Vec(\n",
    "        # NOTE: you can use either a list of strings (sentences)\n",
    "        # or provide a corpus_file\n",
    "        corpus, \n",
    "        \n",
    "        # Dimensionality of the word vectors.\n",
    "        vector_size=VEC_SIZE, \n",
    "        \n",
    "        # Maximum distance between the current and predicted word within a sentence.\n",
    "        window=5, \n",
    "\n",
    "        # Ignores all words with total frequency lower than this.\n",
    "        min_count=30, #1\n",
    "\n",
    "        # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "        sg = 1,\n",
    "        \n",
    "        # Use these many worker threads to train the model (=faster training \n",
    "        # with multicore machines).\n",
    "        workers=32,\n",
    "\n",
    "        # If 0, use the sum of the context word vectors. If 1, use the mean, \n",
    "        # only applies when cbow is used.\n",
    "        cbow_mean = 1,\n",
    "\n",
    "        # Number of iterations (epochs) over the corpus. (Formerly: iter)\n",
    "        epochs=2 #100\n",
    "    )\n",
    "\n",
    "    w2v.add_null_word()\n",
    "\n",
    "    w2v.save(W2V_FILE)\n",
    "\n",
    "else:\n",
    "\n",
    "     w2v = Word2Vec.load(W2V_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_math(model, *args, return_str_description:bool=False):\n",
    "    desc = ''\n",
    "    func = np.add  \n",
    "    # NOTE: vector_size  \n",
    "    vect = None\n",
    "    \n",
    "    for arg in args:\n",
    "        if arg == '+':\n",
    "            desc += ' + '\n",
    "            func = np.add\n",
    "        elif arg == '-':\n",
    "            desc += ' - '\n",
    "            func = np.subtract\n",
    "        elif arg == '/':\n",
    "            desc += ' / '\n",
    "            func = np.divide\n",
    "        elif arg == '*':\n",
    "            desc += ' * '\n",
    "            func = np.multiply\n",
    "        else:\n",
    "            desc += arg\n",
    "            curr = model.wv[arg]\n",
    "            if vect is None:\n",
    "                vect = curr.copy()\n",
    "            else:\n",
    "                vect = func(vect, curr)\n",
    "\n",
    "    if return_str_description:\n",
    "        return desc, vect\n",
    "\n",
    "    return vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple utility function for doing vector math based on a string\n",
    "#def vec_math(model, *args, return_str_description:bool=False):    \n",
    "#    desc = ''\n",
    "#    func = np.add  \n",
    "    # NOTE: vector_size  \n",
    "#    vect = None\n",
    "    \n",
    "#    for arg in args:\n",
    "#        match arg:\n",
    "#            case '+':\n",
    "#                desc+= ' + '\n",
    "#                func = np.add\n",
    "\n",
    "#            case '-':\n",
    "#                desc+= ' - '\n",
    "#                func = np.subtract\n",
    "#                \n",
    "#            case '/':\n",
    "#                desc+= ' / '\n",
    "#                func = np.divide#\n",
    "\n",
    "#            case '*':\n",
    "#                desc+= ' * '\n",
    "#                func = np.multiply\n",
    "\n",
    "#            case _:\n",
    "#                desc += arg\n",
    "#                curr = model.wv[arg]\n",
    "#                if vect is None:\n",
    "#                    vect = curr.copy()\n",
    "#                else:\n",
    "#                    vect = func(vect, curr)\n",
    "\n",
    "#    if return_str_description:\n",
    "#        desc, vect\n",
    "\n",
    "#    return vect\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: here my model had \n",
    "\n",
    "$$\n",
    "woman + man - king = girl\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "news + truth - lies = blogging\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('girl', 0.7620220184326172)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# woman + man - king = ?\n",
    "gsm_res = w2v.wv.most_similar(positive=['woman','man'], negative=['king'])[0]\n",
    "gsm_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('usenet', 0.7321229577064514)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# news + truth - lies = ?\n",
    "gsm_res = w2v.wv.most_similar(positive=['news','truth'], negative=['lies'])[0]\n",
    "gsm_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('headline', 0.9040331244468689),\n",
       " ('overviews', 0.8954185247421265),\n",
       " ('cnn', 0.879298210144043),\n",
       " ('allafrica', 0.8569325804710388),\n",
       " ('blog', 0.8361571431159973),\n",
       " ('pbs', 0.8183717727661133),\n",
       " ('npr', 0.8139971494674683),\n",
       " ('yahoo', 0.8130702376365662),\n",
       " ('herald', 0.8103915452957153),\n",
       " ('bbc', 0.8064829707145691)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('news')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Plot these word embeddings (all words) by adjusting the code in the notebook.\n",
    "\n",
    "You are able to adjust the plotting parameters to suit your needs for making a compelling visualization. \n",
    "Discuss what you notice in your embeddings. For example, using the introduction to Charles Darwin's \"_On the Origin of Species_\" \n",
    "as a text file, we obtain the embeddings in Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/dnjthp912l1b5jz35cfdys0r0000gn/T/ipykernel_82613/2146309385.py:23: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  ax.set_xlim(min_vals[0] * lim_scale, max_vals[0] * lim_scale)\n",
      "/var/folders/yk/dnjthp912l1b5jz35cfdys0r0000gn/T/ipykernel_82613/2146309385.py:24: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  ax.set_ylim(min_vals[1] * lim_scale, max_vals[1] * lim_scale)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: modified code from below here\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "lim_scale = 2\n",
    "\n",
    "word_groups = [\n",
    "    'originated,abuse'.split(','), \n",
    "    'anarchism'.split(','), \n",
    "]\n",
    "\n",
    "for i, words in enumerate(word_groups):\n",
    "    ax = fig.add_subplot(1, 2, i+1)\n",
    "    single_words = list(filter(lambda e: e.isalpha(), words))\n",
    "    domath_words = list(filter(lambda e: not e.isalpha(), words))\n",
    "\n",
    "    vecs = w2v.wv[single_words]\n",
    "    for w in domath_words:        \n",
    "        v = vec_math(w2v, *w.split())\n",
    "        vecs = np.vstack((vecs, v))      \n",
    "\n",
    "    min_vals = np.min(vecs, axis=0)\n",
    "    max_vals = np.max(vecs, axis=0)  \n",
    "\n",
    "    ax.set_xlim(min_vals[0] * lim_scale, max_vals[0] * lim_scale)\n",
    "    ax.set_ylim(min_vals[1] * lim_scale, max_vals[1] * lim_scale)\n",
    "\n",
    "    for j, vec in enumerate(vecs):\n",
    "        word = words[j]\n",
    "\n",
    "        plt.scatter(vec[0], vec[1])\n",
    "        plt.annotate(\n",
    "            word, xy=(0, 0),  xytext=(vec[0], vec[1]), \n",
    "            arrowprops=dict(arrowstyle=\"<-\")\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Since this is too crowded to interpret, modify the code in the notebook to randomly select words to plot as long as there is space as shown in Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'blogging' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m single_words \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m e: e\u001b[39m.\u001b[39misalpha(), words))\n\u001b[1;32m     12\u001b[0m domath_words \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m e: \u001b[39mnot\u001b[39;00m e\u001b[39m.\u001b[39misalpha(), words))\n\u001b[0;32m---> 14\u001b[0m vecs \u001b[39m=\u001b[39m w2v\u001b[39m.\u001b[39;49mwv[single_words]\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m domath_words:        \n\u001b[1;32m     16\u001b[0m     v \u001b[39m=\u001b[39m vec_math(w2v, \u001b[39m*\u001b[39mw\u001b[39m.\u001b[39msplit())\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_2023/lib/python3.9/site-packages/gensim/models/keyedvectors.py:405\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m    403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key_or_keys)\n\u001b[0;32m--> 405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_2023/lib/python3.9/site-packages/gensim/models/keyedvectors.py:405\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m    403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key_or_keys)\n\u001b[0;32m--> 405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_2023/lib/python3.9/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning_2023/lib/python3.9/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'blogging' not present\""
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "lim_scale = 2\n",
    "\n",
    "word_groups = [\n",
    "    'computer,news,truth,lies,blogging,news + truth - lies'.split(','), \n",
    "    'man,king,woman,queen,woman + man - king'.split(','), \n",
    "]\n",
    "\n",
    "for i, words in enumerate(word_groups):\n",
    "    ax = fig.add_subplot(1, 2, i+1)\n",
    "    single_words = list(filter(lambda e: e.isalpha(), words))\n",
    "    domath_words = list(filter(lambda e: not e.isalpha(), words))\n",
    "\n",
    "    vecs = w2v.wv[single_words]\n",
    "    for w in domath_words:        \n",
    "        v = vec_math(w2v, *w.split())\n",
    "        vecs = np.vstack((vecs, v))      \n",
    "\n",
    "    min_vals = np.min(vecs, axis=0)\n",
    "    max_vals = np.max(vecs, axis=0)  \n",
    "\n",
    "    ax.set_xlim(min_vals[0] * lim_scale, max_vals[0] * lim_scale)\n",
    "    ax.set_ylim(min_vals[1] * lim_scale, max_vals[1] * lim_scale)\n",
    "\n",
    "    for j, vec in enumerate(vecs):\n",
    "        word = words[j]\n",
    "\n",
    "        plt.scatter(vec[0], vec[1])\n",
    "        plt.annotate(\n",
    "            word, xy=(0, 0),  xytext=(vec[0], vec[1]), \n",
    "            arrowprops=dict(arrowstyle=\"<-\")\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether or not you used `gensim` for training the `Word2Vec` model you will be using PyTorch Lightning for the `Seq2Seq` model. To make this easier on you, you will need to create a `data.Dataset` object which contains, at the very least, `__getitem__` and `__len__` methods to yield items during training. Below is an example which converts a `gensim` corpus paired batches of the first 300 words per doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTextDataset(data.Dataset):\n",
    "    def __init__(self, corpus, keyed_vecs, reverse_target:bool=False):        \n",
    "        self.corpus = corpus\n",
    "        self.keyed_vecs = keyed_vecs\n",
    "        \n",
    "        docs = self.docs_from_corpus(corpus)\n",
    "        self.docs = docs\n",
    "        self.data = docs\n",
    "\n",
    "        corpora_dct = corpora.Dictionary(docs)\n",
    "        self.corpora_dct = corpora_dct\n",
    "\n",
    "        bows = self.bows_from_docs(docs, corpora_dct)\n",
    "        self.bows = bows\n",
    "        \n",
    "        self.reverse_target = reverse_target\n",
    "\n",
    "    def docs_from_corpus(self, corpus):\n",
    "        docs = [\n",
    "            simple_preprocess(remove_stopwords(' '.join(doc)))[:300] \n",
    "            for doc in corpus\n",
    "        ]\n",
    "        return docs\n",
    "        \n",
    "    def bows_from_docs(self, docs, dct):\n",
    "        return [dct.doc2bow(doc) for doc in docs]\n",
    "\n",
    "\n",
    "    def __len__(self):        \n",
    "        return int(len(self.docs))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.docs[idx]\n",
    "\n",
    "        source = torch.tensor([self.keyed_vecs.get_index(word) for word in doc])      \n",
    "        \n",
    "        target = [self.keyed_vecs.get_index(word) for word in doc]\n",
    "        if self.reverse_target:\n",
    "            target = target[::-1]\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        return source, target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: fill in the `encode` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_vocabsize:int, n_embedding:int, n_hidden:int, n_layers:int, \n",
    "        dropout:Optional[float]=0.1, pretrained:Optional[np.ndarray]=None,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "                \n",
    "        self.n_vocabsize = n_vocabsize\n",
    "        self.n_embedding = n_embedding\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        embedding = self.make_embedding_layer(n_vocabsize, n_embedding, pretrained)\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # NOTE: depending on your dataset you may have to change batch_first\n",
    "        self.lstm = nn.LSTM(n_embedding, n_hidden, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def make_embedding_layer(\n",
    "        self, \n",
    "        n_vocabsize:int, \n",
    "        n_embedding:int,\n",
    "        pretrained:Optional[np.ndarray]=None,\n",
    "    ):\n",
    "        if pretrained is not None:\n",
    "            pretrained = np.array(pretrained)\n",
    "            n_vocabsize, n_embedding = pretrained.shape\n",
    "            \n",
    "            self.n_vocabsize = n_vocabsize\n",
    "            self.n_embedding = n_embedding\n",
    "\n",
    "            pretrained = torch.FloatTensor(pretrained) \n",
    "            embedding = nn.Embedding.from_pretrained(pretrained)\n",
    "\n",
    "        else:\n",
    "            embedding = nn.Embedding(n_vocabsize, n_embedding)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "\n",
    "    def encode(self, x):\n",
    "       # TODO: fill this in\n",
    "       pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encode(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: fill in the `decode` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocabsize:int, n_embedding:int, n_hidden:int, n_layers:int, \n",
    "        dropout:Optional[float]=0.1,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_vocabsize = n_vocabsize\n",
    "        self.n_embedding = n_embedding\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocabsize, n_embedding)\n",
    "        self.lstm = nn.LSTM(n_embedding, n_hidden, n_layers, dropout=dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(n_hidden, n_vocabsize)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def decode(self, x, hidden, cell):\n",
    "        # TODO: fill this in\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        return self.decode(x, hidden, cell)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq LightningModule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: fill in the `do_seq2seq` method and define the `criterion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(pl.LightningModule):    \n",
    "    def __init__(\n",
    "        self, \n",
    "        word_2_vec,\n",
    "        train_loader,\n",
    "        n_vocabsize:int=0, \n",
    "        n_hidden:int = 2,\n",
    "        n_layers:int = 2,\n",
    "        dropout:float = 0.2,\n",
    "        teacher_forcing_ratio:float=0.5,\n",
    "        learning_rate:float=0.01,\n",
    "    ):\n",
    "                \n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.w2v = word_2_vec\n",
    "        self.train_loader = train_loader\n",
    "\n",
    "        n_embedding = word_2_vec.vector_size\n",
    "        n_vocabsize, n_embedding = np.array(word_2_vec.wv).shape\n",
    "\n",
    "        self.n_vocabsize = n_vocabsize\n",
    "        self.n_embedding = n_embedding\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.n_output = n_vocabsize\n",
    "\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            n_vocabsize, n_embedding, n_hidden, n_layers, \n",
    "            dropout, pretrained=np.array(word_2_vec.wv)\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            n_vocabsize, n_embedding, n_hidden, n_layers, dropout,\n",
    "        )\n",
    "\n",
    "        # TODO: self.criterion = loss_fn\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def encode(self, word):\n",
    "        index = self.w2v.wv.get_index(word)\n",
    "        return torch.tensor(index)\n",
    "    \n",
    "    def do_seq2seq(self, source, target):\n",
    "        # TODO: fill this in\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        source = x\n",
    "        target = y\n",
    "\n",
    "        output = self.do_seq2seq(source, target)\n",
    "\n",
    "        # NOTE: this may not be needed depending on your\n",
    "        output_dim = output.shape[-1]        \n",
    "        output = output.view(-1, output_dim)\n",
    "        target = target.view(-1)\n",
    "\n",
    "\n",
    "        loss = self.criterion(output, target)\n",
    "\n",
    "        result = {'loss': loss}\n",
    "        self.log('loss', loss)\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate dataset and dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Seq2SeqTextDataset(corpus, w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data.DataLoader(ds, batch_size=3, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(253854, 50)\n",
       "    (lstm): LSTM(50, 12, num_layers=4, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(253854, 50)\n",
       "    (lstm): LSTM(50, 12, num_layers=4, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=12, out_features=253854, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2s = Seq2Seq(\n",
    "    w2v, dl,\n",
    "    n_hidden=12, n_layers=4, dropout=0.5\n",
    ").to(DEVICE)\n",
    "s2s.encoder.to(DEVICE)\n",
    "s2s.decoder.to(DEVICE)\n",
    "s2s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/solstice/anaconda3/envs/lstm/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    \n",
    "    # NOTE: gradient clipping can help prevent exploding gradients\n",
    "    gradient_clip_val=100,\n",
    "    gradient_clip_algorithm='value',    \n",
    "    log_every_n_steps=5,\n",
    "    \n",
    "    # NOTE: this should match your device i.e. if you set cuda above, this should be cuda. \n",
    "    # Otherwise it should be cpu. \n",
    "    accelerator=DEVICE,    \n",
    "    \n",
    "    # NOTE: you can set the maximum time you want to train your model\n",
    "    max_time={'minutes': 2},    \n",
    "\n",
    "    # NOTE: setting this to true will save your model every so often\n",
    "    enable_checkpointing=False,\n",
    "    accumulate_grad_batches=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | Encoder          | 12.7 M\n",
      "1 | decoder   | Decoder          | 16.0 M\n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "16.0 M    Trainable params\n",
      "12.7 M    Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.797   Total estimated model params size (MB)\n",
      "/home/solstice/anaconda3/envs/lstm/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918da39c6069419182fa0d5696a1e2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time limit reached. Elapsed time is 0:02:25. Signaling Trainer to stop.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(s2s, dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Train your network also to reconstruct sentences with some words masked. How well does this training perform? Report accuracy at convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, target in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 253854])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = s2s.do_seq2seq(\n",
    "    source[0, :].unsqueeze(0),\n",
    "    target[0, :].unsqueeze(0)\n",
    ")\n",
    "res = res.squeeze(1)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w2v.wv.index_to_key[idx] for idx in res.argmax(1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Put in a sentence from the corpus and write its reconstruction. Is the reconstruction perfect?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Now mask one of the words in the sentence with xxxx and test if it fills the word back."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Finally, start a sentence from the corpus by giving the first 3 words, with the rest of the words masked, see if it completes this sentence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `gensim` to load other models aside from `Word2Vec` such as `glove` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim_api.load('glove-twitter-25')\n",
    "glove_vectors.most_similar('twitter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook adapts parts of [@Ben Trevett](https://github.com/bentrevett)'s PyTorch Seq2Seq [notebook](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb). Additionally we utilize parts of both PyTorch's [documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) as well as [Gensim](https://radimrehurek.com/gensim/intro.html)'s and [Machine Learning Plus](https://www.machinelearningplus.com/)'s [tutorial](https://www.machinelearningplus.com/nlp/gensim-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those getting started with [`gensim`](https://radimrehurek.com/gensim/intro.html) they may find [Gaurav Padawe](https://medium.com/@grvpdw92) [tutorial](https://medium.com/analytics-vidhya/word2vector-using-gensim-e055d35f1cb4) useful. Of course there is also the official `Word2Vec` `gensim` [tutorial](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "For support with [Pylightning](https://lightning.ai/docs/pytorch/stable/) please refer to these documentation pages:\n",
    "- `Trainer` [documentation](https://lightning.ai/docs/pytorch/stable/common/trainer.html)\n",
    "- `LightningModule` [documentation](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48405d14813ffd37f8be7251e92423799cab262faebc8080a71f67eaa8a9635a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
