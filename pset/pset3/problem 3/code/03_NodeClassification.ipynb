{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data handling code is adapted from the PyTorch geometric collection of google colab notebooks, a fantastic resource for getting started with GNNs. https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.data import DataLoader\n",
    "# import the graph classifier you built in the last step\n",
    "from GCN_03 import NodeClassifier, NodeClassifierWelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "====================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "=============================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# - - - DATA PREPARATIONS - - -\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of tensor a (2708) must match the size of tensor b (13264) at non-singleton dimension 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out[data.train_mask],\n",
    "                     data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "    return test_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = NodeClassifier(num_node_features=1433, hidden_features=16, num_classes=7)\n",
    "optimizer = torch.optim.Adam(model_new.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.9207, Test Accuracy: 0.138\n",
      "Epoch: 020, Loss: 1.7281, Test Accuracy: 0.391\n",
      "Epoch: 030, Loss: 1.4046, Test Accuracy: 0.47\n",
      "Epoch: 040, Loss: 1.1288, Test Accuracy: 0.516\n",
      "Epoch: 050, Loss: 0.8131, Test Accuracy: 0.576\n",
      "Epoch: 060, Loss: 0.4540, Test Accuracy: 0.688\n",
      "Epoch: 070, Loss: 0.3599, Test Accuracy: 0.701\n",
      "Epoch: 080, Loss: 0.2191, Test Accuracy: 0.696\n",
      "Epoch: 090, Loss: 0.1600, Test Accuracy: 0.697\n",
      "Epoch: 100, Loss: 0.1501, Test Accuracy: 0.682\n",
      "Epoch: 110, Loss: 0.0879, Test Accuracy: 0.686\n",
      "Epoch: 120, Loss: 0.1622, Test Accuracy: 0.679\n",
      "Epoch: 130, Loss: 0.0675, Test Accuracy: 0.652\n",
      "Epoch: 140, Loss: 0.0855, Test Accuracy: 0.693\n",
      "Epoch: 150, Loss: 0.0419, Test Accuracy: 0.683\n",
      "Epoch: 160, Loss: 0.0650, Test Accuracy: 0.676\n",
      "Epoch: 170, Loss: 0.0268, Test Accuracy: 0.684\n",
      "Epoch: 180, Loss: 0.0425, Test Accuracy: 0.66\n",
      "Epoch: 190, Loss: 0.0555, Test Accuracy: 0.691\n",
      "Epoch: 200, Loss: 0.0210, Test Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train(model_new)\n",
    "    if epoch % 10 == 0:\n",
    "        test_acc = test(model_new)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Accuracy: {test_acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old model (Welling et al., 2011) for node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_welling = NodeClassifierWelling(num_node_features=1433, hidden_features=16, num_classes=7)\n",
    "optimizer = torch.optim.Adam(model_welling.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.9051, Test Accuracy: 0.26\n",
      "Epoch: 020, Loss: 1.7933, Test Accuracy: 0.397\n",
      "Epoch: 030, Loss: 1.6306, Test Accuracy: 0.487\n",
      "Epoch: 040, Loss: 1.3842, Test Accuracy: 0.574\n",
      "Epoch: 050, Loss: 1.1442, Test Accuracy: 0.642\n",
      "Epoch: 060, Loss: 0.9415, Test Accuracy: 0.699\n",
      "Epoch: 070, Loss: 0.7291, Test Accuracy: 0.732\n",
      "Epoch: 080, Loss: 0.5568, Test Accuracy: 0.759\n",
      "Epoch: 090, Loss: 0.4906, Test Accuracy: 0.767\n",
      "Epoch: 100, Loss: 0.3739, Test Accuracy: 0.772\n",
      "Epoch: 110, Loss: 0.2850, Test Accuracy: 0.771\n",
      "Epoch: 120, Loss: 0.2750, Test Accuracy: 0.773\n",
      "Epoch: 130, Loss: 0.2425, Test Accuracy: 0.774\n",
      "Epoch: 140, Loss: 0.1538, Test Accuracy: 0.773\n",
      "Epoch: 150, Loss: 0.1688, Test Accuracy: 0.78\n",
      "Epoch: 160, Loss: 0.1452, Test Accuracy: 0.774\n",
      "Epoch: 170, Loss: 0.1105, Test Accuracy: 0.778\n",
      "Epoch: 180, Loss: 0.1324, Test Accuracy: 0.774\n",
      "Epoch: 190, Loss: 0.1507, Test Accuracy: 0.773\n",
      "Epoch: 200, Loss: 0.1040, Test Accuracy: 0.772\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train(model_welling)\n",
    "    if epoch % 10 == 0:\n",
    "        test_acc = test(model_welling)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Accuracy: {test_acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model model has lower loss, but also lower accuracy. Increasing the size of Multilayer perceptron might increase the expressive power of the model and improve the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
